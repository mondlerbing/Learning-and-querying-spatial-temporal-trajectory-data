{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd0d1d6",
   "metadata": {
    "id": "7dd0d1d6"
   },
   "source": [
    "# DSCI 521: Data Analysis and Interpretation <br> Term Project Phase 1: Scoping an analytics project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86edb44",
   "metadata": {
    "id": "d86edb44"
   },
   "source": [
    "### Project submission group\n",
    "- Group member 1\n",
    "    - Name: Jaz Zhou\n",
    "    - Email: jz878@drexel.edu\n",
    "\n",
    "\n",
    "### Additional submission comments\n",
    "- Tutoring support received: NA\n",
    "- Other (other): NA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768a3709",
   "metadata": {
    "id": "768a3709"
   },
   "source": [
    "## 1. Abstract\n",
    "This project explores the topic of smart cities within the realm of urban design, specifically, focusing on understanding and analyzing taxi trajectories in the lights of understanding this particular transportation pattern, and maybe developed into similarity search and/or prediction model.\n",
    "\n",
    "We utilized a Kaggle dataset, specifically the \"train.csv\" file with 1.7 million taxi trajectories, averaging 50 points each. Additionally, the \"test.csv\" file provided snapshots of ongoing trips at specific timestamps (14/08/2014 18:00:00, 30/09/2014 08:30:00, 06/10/2014 17:45:00, 01/11/2014 04:00:00, and 21/12/2014 14:30:00).\n",
    "\n",
    "The exploratory data analysis offers initial insights for in-depth analysis model.\n",
    "\n",
    "**Key findings include**:\n",
    "\n",
    "- **\"DAY_TYPE\" data revised**: addressing concerns about the reliability of the \"DAYTYPE\" feature. Mostly showing type 'A' days, we revised it using a holiday list for more accurate time pattern representation.\n",
    "\n",
    "- **Popular Stand Existence**: Stand 15 emerged as the most frequented, crucial for resource optimization and service efficiency.\n",
    "\n",
    "- **Temporal Patterns**: Distinct peaks around 8:50 AM and local peaks at sharp hours provide insights for effective fleet management.\n",
    "\n",
    "- **Trajectory Length**: Median trajectory length is 4.69 km, varying by call type.\n",
    "\n",
    "- **Sampling Rate**: Moderate variability in taxi trajectory sampling rates(median = 11, mean = 11.9 per taxi per day), with notable variations observed(std = 6.4, min = 1, max = 266 per taxi per day).\n",
    "\n",
    "- **Correlation**: No discernible linear or non-linear correlation among 'CALL_TYPE,' 'TAXI_ID,' 'DAY_TYPE_REVISED,' and 'TIMESTAMP,' indicating independence of features.\n",
    "\n",
    "Acknowledging dataset limitations, including variations in GPS data completeness and the study's confined scope to a single city for a specific year, ensures a comprehensive understanding within the project's context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d232c0cc",
   "metadata": {
    "id": "d232c0cc"
   },
   "source": [
    "## 2. Identified Skills & Areas needing Growth\n",
    "\n",
    "### Jaz Zhou\n",
    "\n",
    "\n",
    "\n",
    "####  Identified Skills\n",
    "\n",
    "\n",
    "I used to study and work in Architecture and Urban Design, equiping me the ability to identify and leverage relevant data for the continual improvement of urban design and operational processes. My skills encompass:\n",
    "\n",
    "- Utilizing my architectural knowledge to identify key areas of focus in urban design.\n",
    "- Interpreting data through the lens of urban design principles.\n",
    "- Recognizing the application or research value of chosen topic.\n",
    "\n",
    "  \n",
    "####  Areas requiring growth\n",
    "\n",
    "While traditional urban design relies on experience and intuition, the integration of data science opens avenues for quantification and evidence-based decision-making. To enhance my skill set, I am eager to delve deeper into:\n",
    "\n",
    "- Developing proficiency in data visualization to communicate findings effectively to stakeholders.\n",
    "- Exploring various machine learning algorithms such as regression, clustering, and classification to analyze urban patterns.\n",
    "- Delving into deep learning models to uncover complex relationships within urban data, allowing for predictive modeling and scenario analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d31550",
   "metadata": {
    "id": "94d31550"
   },
   "source": [
    "## 3. Topic Choice\n",
    "\n",
    "In the realm of city planning, smart cities are quite a hot topic. One key part is improving how we interpret  traffic patterns using advanced technology and data.\n",
    "\n",
    "Specifically, for this project, we're looking at taxi routes – studying where they go and predicting where they'll go next. This helps us understand popular routes, congestion points, and commuter preferences.\n",
    "\n",
    "**Goals of the Project**:\n",
    "\n",
    "-**Find Similar Routes**:\n",
    "    Employ machine learning algorithms to identify patterns in taxi trajectories. Clustering techniques can group similar routes, highlighting popular paths taken by taxis.\n",
    "    Utilize spatial data analysis to identify common starting and ending points, as well as recurring routes. This information is valuable for understanding commuter behavior.\n",
    "\n",
    "-**Predict Future Routes**:\n",
    "    Train machine learning models to predict future taxi trajectories based on historical data. This can assist in proactive traffic management and resource allocation.\n",
    "    Predicting how traffic conditions might evolve can help in optimizing traffic signal timings and managing road infrastructure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c880277",
   "metadata": {
    "id": "6c880277"
   },
   "source": [
    "## 4. Relevant Pre-processed Data Selection\n",
    "\n",
    "As GPS-enabled devices become widespread, trajectory data is being generated rapidly. We possess a strong dataset sourced from `Kaggle` covering `a full year (01/07/2013 to 30/06/2014)` detailing the `trajectories of 442 taxis in Porto, Portugal`[1]. This valuable information is neatly organized in a single CSV file named \"train.csv,\" allowing for in-depth exploration and insights into the city's transportation dynamics.\n",
    "\n",
    "\n",
    "[1] Meghan O'Connell, moreiraMatias, Wendy Kan, \"ECML/PKDD 15: Taxi Trajectory Prediction (I),\" Kaggle, 2015. [Online]. Available: https://kaggle.com/competitions/pkdd-15-predict-taxi-service-trajectory-i\n",
    "\n",
    "\n",
    "\n",
    "### 4.1 Acquire data\n",
    "\n",
    "To download the data zip file, you can go to the website and download, or install kaggle and register an api to acquire it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7693594",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8562,
     "status": "ok",
     "timestamp": 1709765720819,
     "user": {
      "displayName": "Peizhi WU",
      "userId": "15522070970651565039"
     },
     "user_tz": 300
    },
    "id": "b7693594",
    "outputId": "15f5651a-2f7a-40ee-8142-2d4c688c8148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "zKYT5aSR-f2V",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "executionInfo": {
     "elapsed": 7848,
     "status": "ok",
     "timestamp": 1709765728653,
     "user": {
      "displayName": "Peizhi WU",
      "userId": "15522070970651565039"
     },
     "user_tz": 300
    },
    "id": "zKYT5aSR-f2V",
    "outputId": "3acce845-8df1-49e8-e1c2-068b0f259786"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-57c0de3c-0b4d-4dcd-99d9-6d9ccdf8fc4a\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-57c0de3c-0b4d-4dcd-99d9-6d9ccdf8fc4a\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving kaggle.json to kaggle.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kaggle.json': b'{\"username\":\"jazzhou\",\"key\":\"cc340e8b471b497acec985317a9d90c4\"}'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1YCfJ9XX-gq0",
   "metadata": {
    "executionInfo": {
     "elapsed": 360,
     "status": "ok",
     "timestamp": 1709765731212,
     "user": {
      "displayName": "Peizhi WU",
      "userId": "15522070970651565039"
     },
     "user_tz": 300
    },
    "id": "1YCfJ9XX-gq0"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!mv kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "-iLTJ7cE-i5y",
   "metadata": {
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1709765733705,
     "user": {
      "displayName": "Peizhi WU",
      "userId": "15522070970651565039"
     },
     "user_tz": 300
    },
    "id": "-iLTJ7cE-i5y"
   },
   "outputs": [],
   "source": [
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e8883f",
   "metadata": {
    "id": "67e8883f"
   },
   "source": [
    "after installing the kaggle.\n",
    "- hop to kaggle's website and create an acount\n",
    "- go to acount - settings - API - create a new token\n",
    "- paste the downloaded token `kaggle.json` into `Users/yourUserName/.kaggle/`\n",
    "\n",
    "then run the code below to download the zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d268d2e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5571,
     "status": "ok",
     "timestamp": 1709765742303,
     "user": {
      "displayName": "Peizhi WU",
      "userId": "15522070970651565039"
     },
     "user_tz": 300
    },
    "id": "d268d2e2",
    "outputId": "ba0435a2-ef38-46cc-90c4-d9be2b98de5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading pkdd-15-predict-taxi-service-trajectory-i.zip to /content\n",
      " 98% 498M/509M [00:04<00:00, 131MB/s]\n",
      "100% 509M/509M [00:04<00:00, 126MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c pkdd-15-predict-taxi-service-trajectory-i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adef88f7",
   "metadata": {
    "id": "adef88f7"
   },
   "source": [
    "Next, unzip the zip file and extract the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28b537b0",
   "metadata": {
    "executionInfo": {
     "elapsed": 102,
     "status": "ok",
     "timestamp": 1709765745632,
     "user": {
      "displayName": "Peizhi WU",
      "userId": "15522070970651565039"
     },
     "user_tz": 300
    },
    "id": "28b537b0"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def unzip(zip_file_path, extracted_folder_path):\n",
    "\n",
    "    # Open the zip file\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        # Extract all the contents into the specified folder\n",
    "        zip_ref.extractall(extracted_folder_path)\n",
    "\n",
    "    # Optionally, you can print a message to confirm the extraction\n",
    "    print(f\"File '{zip_file_path}' has been successfully extracted to '{extracted_folder_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bd118ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27497,
     "status": "ok",
     "timestamp": 1709765775042,
     "user": {
      "displayName": "Peizhi WU",
      "userId": "15522070970651565039"
     },
     "user_tz": 300
    },
    "id": "7bd118ea",
    "outputId": "e4c8db52-88ff-440a-f2ec-6bdfb551a786"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'pkdd-15-predict-taxi-service-trajectory-i.zip' has been successfully extracted to 'data'.\n",
      "File 'data/train.csv.zip' has been successfully extracted to 'data/trainData'.\n",
      "File 'data/test.csv.zip' has been successfully extracted to 'data/testData'.\n",
      "File 'data/metaData_taxistandsID_name_GPSlocation.csv.zip' has been successfully extracted to 'data/metaData'.\n"
     ]
    }
   ],
   "source": [
    "unzip('pkdd-15-predict-taxi-service-trajectory-i.zip', 'data')\n",
    "\n",
    "unzip('data/train.csv.zip', 'data/trainData')\n",
    "\n",
    "unzip('data/test.csv.zip', 'data/testData')\n",
    "\n",
    "unzip('data/metaData_taxistandsID_name_GPSlocation.csv.zip', 'data/metaData')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bG-IYvxVUq9V",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1709771518890,
     "user": {
      "displayName": "Peizhi WU",
      "userId": "15522070970651565039"
     },
     "user_tz": 300
    },
    "id": "bG-IYvxVUq9V",
    "outputId": "6159e5cf-c4a2-4e90-df94-94e913594000"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "y9kYvR_WUrjg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 117,
     "status": "ok",
     "timestamp": 1709771586682,
     "user": {
      "displayName": "Peizhi WU",
      "userId": "15522070970651565039"
     },
     "user_tz": 300
    },
    "id": "y9kYvR_WUrjg",
    "outputId": "20b8a1b5-12e9-479f-af24-8a417d896aca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.config', 'pkdd-15-predict-taxi-service-trajectory-i.zip', 'data', 'sample_data']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d263db",
   "metadata": {
    "id": "e5d263db"
   },
   "source": [
    "### 4.2 Raw Data\n",
    "\n",
    "The raw data includes train.csv, test.csv and meta data csv file.\n",
    "\n",
    "**1. train.csv file**\n",
    "\n",
    "train.csv contains 1.7 million taxi trajectories collected from Porto, Portugal, between July 2013\n",
    "and June 2014. The trajectory points do not come with timestamps. The average number of points\n",
    "per trajectory is 50.\n",
    "\n",
    "Each data sample corresponds to one completed trip. It contains a total of 9 (nine) features, described as follows:\n",
    "\n",
    "- **TRIP_ID:** (String) Unique identifier for each trip.\n",
    "- **CALL_TYPE:** (char) Identifies the way used to demand this service. It may contain one of three possible values:\n",
    "  - 'A' if this trip was dispatched from the central.\n",
    "  - 'B' if this trip was demanded directly to a taxi driver on a specific stand.\n",
    "  - 'C' otherwise (i.e. a trip demanded on a random street).\n",
    "- **ORIGIN_CALL:** (integer) Unique identifier for each phone number which was used to demand, at least, one service. It identifies the trip’s customer if CALL_TYPE='A'. Otherwise, it assumes a NULL value.\n",
    "- **ORIGIN_STAND:** (integer) Unique identifier for the taxi stand. It identifies the starting point of the trip if CALL_TYPE='B'. Otherwise, it assumes a NULL value.\n",
    "- **TAXI_ID:** (integer) Unique identifier for the taxi driver that performed each trip.\n",
    "- **TIMESTAMP:** (integer) Unix Timestamp (in seconds). Identifies the trip’s start.\n",
    "- **DAYTYPE:** (char) Identifies the daytype of the trip’s start. It assumes one of three possible values:\n",
    "  - 'B' if this trip started on a holiday or any other special day.\n",
    "  - 'C' if the trip started on a day before a type-B day.\n",
    "  - 'A' otherwise (i.e. a normal day, workday, or weekend).\n",
    "- **MISSING_DATA:** (Boolean) FALSE when the GPS data stream is complete and TRUE whenever one (or more) locations are missing.\n",
    "- **POLYLINE:** (String) Contains a list of GPS coordinates (i.e. WGS84 format) mapped as a string. The beginning and the end of the string are identified with brackets (i.e. [ and ], respectively). Each pair of coordinates is also identified by the same brackets as [LONGITUDE, LATITUDE]. This list contains one pair of coordinates for each 15 seconds of the trip. The last list item corresponds to the trip’s destination while the first one represents its start.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e640057",
   "metadata": {
    "executionInfo": {
     "elapsed": 116,
     "status": "ok",
     "timestamp": 1709768103969,
     "user": {
      "displayName": "Peizhi WU",
      "userId": "15522070970651565039"
     },
     "user_tz": 300
    },
    "id": "9e640057"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def readCsvToDf(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa022d7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 32548,
     "status": "ok",
     "timestamp": 1709768138047,
     "user": {
      "displayName": "Peizhi WU",
      "userId": "15522070970651565039"
     },
     "user_tz": 300
    },
    "id": "fa022d7d",
    "outputId": "48c22735-5bed-4d97-b4e1-e98a149a866b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>CALL_TYPE</th>\n",
       "      <th>ORIGIN_CALL</th>\n",
       "      <th>ORIGIN_STAND</th>\n",
       "      <th>TAXI_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>DAY_TYPE</th>\n",
       "      <th>MISSING_DATA</th>\n",
       "      <th>POLYLINE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1372636858620000589</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000589</td>\n",
       "      <td>1372636858</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.618643,41.141412],[-8.618499,41.141376],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1372637303620000596</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20000596</td>\n",
       "      <td>1372637303</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.639847,41.159826],[-8.640351,41.159871],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1372636951620000320</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000320</td>\n",
       "      <td>1372636951</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.612964,41.140359],[-8.613378,41.14035],[-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1372636854620000520</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000520</td>\n",
       "      <td>1372636854</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.574678,41.151951],[-8.574705,41.151942],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1372637091620000337</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000337</td>\n",
       "      <td>1372637091</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.645994,41.18049],[-8.645949,41.180517],[-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TRIP_ID CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND   TAXI_ID  \\\n",
       "0  1372636858620000589         C          NaN           NaN  20000589   \n",
       "1  1372637303620000596         B          NaN           7.0  20000596   \n",
       "2  1372636951620000320         C          NaN           NaN  20000320   \n",
       "3  1372636854620000520         C          NaN           NaN  20000520   \n",
       "4  1372637091620000337         C          NaN           NaN  20000337   \n",
       "\n",
       "    TIMESTAMP DAY_TYPE  MISSING_DATA  \\\n",
       "0  1372636858        A         False   \n",
       "1  1372637303        A         False   \n",
       "2  1372636951        A         False   \n",
       "3  1372636854        A         False   \n",
       "4  1372637091        A         False   \n",
       "\n",
       "                                            POLYLINE  \n",
       "0  [[-8.618643,41.141412],[-8.618499,41.141376],[...  \n",
       "1  [[-8.639847,41.159826],[-8.640351,41.159871],[...  \n",
       "2  [[-8.612964,41.140359],[-8.613378,41.14035],[-...  \n",
       "3  [[-8.574678,41.151951],[-8.574705,41.151942],[...  \n",
       "4  [[-8.645994,41.18049],[-8.645949,41.180517],[-...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv_path = 'data/trainData/train.csv'\n",
    "train_df = readCsvToDf(train_csv_path)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e1471f",
   "metadata": {
    "id": "e3e1471f"
   },
   "source": [
    "**2. test.csv file**\n",
    "\n",
    "test.csv includes five test sets. Each one of these datasets refer to trips that occurred between 01/07/2014 and 31/12/2014. Each one of these data sets will provide a snapshot of the current network status on a given timestamp. It will provide partial trajectories for each one of the on-going trips during that specific moment.\n",
    "\n",
    "The five snapshots included on the test set refer to the following timestamps:\n",
    "\n",
    "- 14/08/2014 18:00:00\n",
    "\n",
    "- 30/09/2014 08:30:00\n",
    "\n",
    "- 06/10/2014 17:45:00\n",
    "\n",
    "- 01/11/2014 04:00:00\n",
    "- 21/12/2014 14:30:00\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9bbe5c93",
   "metadata": {
    "id": "9bbe5c93",
    "outputId": "56b63c26-1698-4b55-f3a9-5913715ea09b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>CALL_TYPE</th>\n",
       "      <th>ORIGIN_CALL</th>\n",
       "      <th>ORIGIN_STAND</th>\n",
       "      <th>TAXI_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>DAY_TYPE</th>\n",
       "      <th>MISSING_DATA</th>\n",
       "      <th>POLYLINE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20000542</td>\n",
       "      <td>1408039037</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.585676,41.148522],[-8.585712,41.148639],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T2</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.0</td>\n",
       "      <td>20000108</td>\n",
       "      <td>1408038611</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.610876,41.14557],[-8.610858,41.145579],[-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T3</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20000370</td>\n",
       "      <td>1408038568</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.585739,41.148558],[-8.58573,41.148828],[-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T4</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>20000492</td>\n",
       "      <td>1408039090</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.613963,41.141169],[-8.614125,41.141124],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T5</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20000621</td>\n",
       "      <td>1408039177</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.619903,41.148036],[-8.619894,41.148036]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TRIP_ID CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND   TAXI_ID   TIMESTAMP DAY_TYPE  \\\n",
       "0      T1         B          NaN          15.0  20000542  1408039037        A   \n",
       "1      T2         B          NaN          57.0  20000108  1408038611        A   \n",
       "2      T3         B          NaN          15.0  20000370  1408038568        A   \n",
       "3      T4         B          NaN          53.0  20000492  1408039090        A   \n",
       "4      T5         B          NaN          18.0  20000621  1408039177        A   \n",
       "\n",
       "   MISSING_DATA                                           POLYLINE  \n",
       "0         False  [[-8.585676,41.148522],[-8.585712,41.148639],[...  \n",
       "1         False  [[-8.610876,41.14557],[-8.610858,41.145579],[-...  \n",
       "2         False  [[-8.585739,41.148558],[-8.58573,41.148828],[-...  \n",
       "3         False  [[-8.613963,41.141169],[-8.614125,41.141124],[...  \n",
       "4         False      [[-8.619903,41.148036],[-8.619894,41.148036]]  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_csv_path = 'data/testData/test.csv'\n",
    "test_df = readCsvToDf(test_csv_path)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b619d",
   "metadata": {
    "id": "329b619d"
   },
   "source": [
    "**3. metaData.csv file**\n",
    "\n",
    "includes taxi `stand id name` and `Latitude`, `Longitude`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30418c7c",
   "metadata": {
    "id": "30418c7c",
    "outputId": "a5136178-37c6-4fe0-c5ed-2a6e1784c687"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Descricao</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Agra</td>\n",
       "      <td>41.1771457135</td>\n",
       "      <td>-8.609670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>41.15618964</td>\n",
       "      <td>-8.591064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Aldoar</td>\n",
       "      <td>41.1705249231</td>\n",
       "      <td>-8.665876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Alfândega</td>\n",
       "      <td>41.1437639911</td>\n",
       "      <td>-8.621803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Amial</td>\n",
       "      <td>41.1835097223</td>\n",
       "      <td>-8.612726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Descricao       Latitude  Longitude\n",
       "0   1       Agra  41.1771457135  -8.609670\n",
       "1   2    Alameda    41.15618964  -8.591064\n",
       "2   3     Aldoar  41.1705249231  -8.665876\n",
       "3   4  Alfândega  41.1437639911  -8.621803\n",
       "4   5      Amial  41.1835097223  -8.612726"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_csv_path = 'data/metaData/metaData_taxistandsID_name_GPSlocation.csv'\n",
    "meta_df = readCsvToDf(meta_csv_path)\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281-SgFl29C_",
   "metadata": {
    "id": "281-SgFl29C_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "lSzVt6im3TTr",
   "metadata": {
    "id": "lSzVt6im3TTr"
   },
   "source": [
    "\n",
    "### Learn the one-hot vector representation of coordinates\n",
    "\n",
    "To transform continuous GPS coordinates into discrete tokens for more effective data analysis, we convert continuous longitude and latitude coordinates into tokens. We split the space into equal-sized cells, treating each cell as a token. This means all sample points in the same cell get mapped to the same token.\n",
    "\n",
    "* Cell size: The default cell size in the experiments is 100 meters.\n",
    "\n",
    "**Benefits of This Approach:**\n",
    "- **Dimensionality Reduction:** Simplifies the complex input space of continuous coordinates into a manageable set of discrete tokens.\n",
    "- **Enhanced Spatial Analysis:** Facilitates the identification of spatial relationships and patterns that may not be evident in continuous data.\n",
    "- **Application of NLP Techniques:** Enables the use of NLP methodologies, like RNNs and LSTMs, for analyzing spatial data by treating trajectories as sequences of tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fa7703",
   "metadata": {},
   "source": [
    "### get the boundary box of Porto\n",
    "To begin, we define the boundaries of the cells, ensuring a buffer of 0.1 units around the city of Porto. This buffer helps mitigate the impact of noisy coordinates from raw trajectories, preventing them from excessively enlarging the cells' dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca997a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding box for Porto: (41.0494512, 41.2494512, -8.7107884, -8.510788400000001)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"500\"\n",
       "            src=\"map.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f8969b8d640>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import folium\n",
    "from geopy.geocoders import Nominatim\n",
    "from IPython.display import IFrame\n",
    "\n",
    "def get_bounding_box(city_name, buffer=0.1):\n",
    "    geolocator = Nominatim(user_agent=\"city_bounding_box\")\n",
    "    location = geolocator.geocode(city_name)\n",
    "\n",
    "    if location is None:\n",
    "        return None\n",
    "\n",
    "    # Extracting coordinates\n",
    "    lat = location.latitude\n",
    "    lon = location.longitude\n",
    "\n",
    "    # Calculating bounding box\n",
    "    lat_min_boundary = lat - buffer\n",
    "    lat_max_boundary = lat + buffer\n",
    "    lon_min_boundary = lon - buffer\n",
    "    lon_max_boundary = lon + buffer\n",
    "\n",
    "    return lat_min_boundary, lat_max_boundary, lon_min_boundary, lon_max_boundary\n",
    "\n",
    "city_name = \"Porto\"\n",
    "bounding_box = get_bounding_box(city_name)\n",
    "\n",
    "if bounding_box:\n",
    "    print(f\"Bounding box for {city_name}: {bounding_box}\")\n",
    "else:\n",
    "    print(f\"Could not find coordinates for {city_name}\")\n",
    "\n",
    "# Display the map\n",
    "if bounding_box:\n",
    "    map_center = [(bounding_box[0] + bounding_box[1]) / 2, (bounding_box[2] + bounding_box[3]) / 2]\n",
    "    m = folium.Map(location=map_center, zoom_start=10)\n",
    "    folium.Rectangle(bounds=[(bounding_box[0], bounding_box[2]), (bounding_box[1], bounding_box[3])],\n",
    "                      color='blue',\n",
    "                      fill_opacity=0.1).add_to(m)\n",
    "    map_name = \"map.html\"\n",
    "    m.save(map_name)\n",
    "    display(IFrame(src=map_name, width=1200, height=500))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70db1e82",
   "metadata": {},
   "source": [
    "### remove trajectories that are out of boundary\n",
    "We remove trajectories that fall outside the boundary of Porto. Out of 1,710,670 trajectories in the training data, 1,687,544 remain within the boundary. This simplifies the data handling process, focusing only on trajectories relevant to the defined area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "Ae2MvzTJFJiA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1069728,
     "status": "ok",
     "timestamp": 1709776708329,
     "user": {
      "displayName": "Peizhi WU",
      "userId": "15522070970651565039"
     },
     "user_tz": 300
    },
    "id": "Ae2MvzTJFJiA",
    "outputId": "fb16b01b-f0d5-450e-d664-7997e56ab859"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "def drop_trajectory_outside_boundary(df, boundary_box):\n",
    "    \"\"\"\n",
    "    Filters rows from the input DataFrame based on coordinate conditions.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame containing the 'POLYLINE' column.\n",
    "    boundary_box (tuple): Tuple containing the boundary box coordinates (lat_min, lat_max, lon_min, lon_max).\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Filtered DataFrame with rows dropped based on coordinate conditions.\n",
    "    \"\"\"\n",
    "    filtered_rows = []\n",
    "    count = 0\n",
    "    for index, row in df.iterrows():\n",
    "        polyline_list = ast.literal_eval(row['POLYLINE'])\n",
    "        drop_row = False\n",
    "\n",
    "        for coord in polyline_list:\n",
    "            lon, lat = coord\n",
    "            # Check if coordinate is outside the bounding box\n",
    "            if not (boundary_box[0] <= lat <= boundary_box[1] and boundary_box[2] <= lon <= boundary_box[3]):\n",
    "                count += 1\n",
    "                if (count%100 == 0):\n",
    "                    print(\".\", end=\"\")\n",
    "                drop_row = True\n",
    "                break\n",
    "\n",
    "        if not drop_row:\n",
    "            filtered_rows.append(row)\n",
    "\n",
    "    filtered_df = pd.DataFrame(filtered_rows)\n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50be6751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1710670, 9)\n",
      "Dropping trajectory with noisy coordinates\n",
      ".......................................................................................................................................................................................................................................\n",
      "(1687544, 9)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(\"Dropping trajectory outside boundary for train data\", end=\"\\n\")\n",
    "filtered_train_df = drop_trajectory_outside_boundary(train_df, bounding_box)\n",
    "print()\n",
    "print(filtered_train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e97d5a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train_df.to_csv('data/trainData/filtered_train_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dd5a0b",
   "metadata": {},
   "source": [
    "### get the cell grids dimensions\n",
    "Based on the boundary box and the specified cell size, the dimensions of the cell grid are determined to be 223 cells wide and 168 cells tall. This means that the geographical area encompassing Porto has been discretized into a grid consisting of 223 columns and 168 rows of cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "050CteYVFJnU",
   "metadata": {
    "executionInfo": {
     "elapsed": 1948,
     "status": "ok",
     "timestamp": 1709777320361,
     "user": {
      "displayName": "Peizhi WU",
      "userId": "15522070970651565039"
     },
     "user_tz": 300
    },
    "id": "050CteYVFJnU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def calculate_cell_dimensions(boundary_box, cell_size_meters=100):\n",
    "    \"\"\"\n",
    "    Calculate the number of rows and columns in the grid.\n",
    "\n",
    "    Parameters:\n",
    "    - boundary_box: Spatial bounds of the area.\n",
    "    - cell_size_meters: Size of each cell in meters.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple (num_rows, num_cols): Dimensions of the grid.\n",
    "    \"\"\"\n",
    "\n",
    "    min_lat = boundary_box[0]\n",
    "    max_lat = boundary_box[1]\n",
    "    min_lon = boundary_box[2]\n",
    "    max_lon = boundary_box[3]\n",
    "   \n",
    "    \n",
    "    # Earth's radius in meters and conversion factor from degrees to meters\n",
    "    R = 6371000\n",
    "    m_per_deg_lat = R * np.pi / 180\n",
    "    avg_lat = np.radians((min_lat + max_lat) / 2)\n",
    "    m_per_deg_lon = R * np.cos(avg_lat) * np.pi / 180\n",
    "\n",
    "    # Calculate grid dimensions\n",
    "    \n",
    "    lat_extent_m = (max_lat - min_lat) * m_per_deg_lat\n",
    "    lon_extent_m = (max_lon - min_lon) * m_per_deg_lon\n",
    "    num_rows = int(np.ceil(lat_extent_m / cell_size_meters))\n",
    "    num_cols = int(np.ceil(lon_extent_m / cell_size_meters))\n",
    "\n",
    "    return num_rows, num_cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "TdViGjsrH-Ry",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1709776994558,
     "user": {
      "displayName": "Peizhi WU",
      "userId": "15522070970651565039"
     },
     "user_tz": 300
    },
    "id": "TdViGjsrH-Ry",
    "outputId": "99be9d66-0a6e-4ca1-80be-e4329a265986"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223 168\n"
     ]
    }
   ],
   "source": [
    "# Calculate Grid Dimensions\n",
    "\n",
    "num_rows, num_cols = calculate_grid_dimensions(bounding_box)\n",
    "print(num_rows, num_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a5f3aa",
   "metadata": {},
   "source": [
    "### converting raw trajectory into one-hot vector representations\n",
    "We're converting the raw trajectory data into one-hot vector representations, where each point is encoded with only the cell ID. Instead of saving the complete sparse vectors, we'll store only the cell IDs in JSON files. \n",
    "\n",
    "To keep things manageable, we're slicing trajectories into chunks of 100,000 and saving each chunk separately. This helps prevent any issues with system resources and keeps the process safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c499ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def polyline_to_onehot(polyline_series, gps_to_cell_index_func, min_lon, min_lat, max_lon, max_lat, num_rows, num_cols):\n",
    "    \"\"\"\n",
    "    Convert a series of polyline strings to lists of one-hot encoded vectors.\n",
    "\n",
    "    Parameters:\n",
    "    - polyline_series (pandas.Series): Series containing POLYLINE strings with GPS coordinates.\n",
    "    - gps_to_cell_index_func (function): Function that maps GPS coordinates to a cell index.\n",
    "    - min_lon, min_lat, max_lon, max_lat (floats): Spatial bounds of the grid.\n",
    "    - num_rows, num_cols (int): Dimensions of the grid (number of rows and columns).\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    - pandas.Series: A Series where each element is a list of one-hot encoded vectors representing the trajectory.\n",
    "    \"\"\"\n",
    "\n",
    "    one_hot_series = polyline_series.apply(lambda polyline_str: _polyline_str_to_onehot(polyline_str, gps_to_cell_index_func, min_lon, min_lat, max_lon, max_lat, num_rows, num_cols))\n",
    "    return one_hot_series\n",
    "\n",
    "def _polyline_str_to_onehot(polyline_str, gps_to_cell_index_func, min_lon, min_lat, max_lon, max_lat, num_rows, num_cols):\n",
    "    gps_coordinates = json.loads(polyline_str)\n",
    "    one_hot_vectors = []\n",
    "    for lon, lat in gps_coordinates:\n",
    "        cell_index = gps_to_cell_index_func(lon, lat, min_lon, min_lat, num_rows, num_cols)        \n",
    "        one_hot_vectors.append(cell_index)\n",
    "    return one_hot_vectors\n",
    "\n",
    "def gps_to_cell_index(lon, lat, min_lon, min_lat, num_rows, num_cols, cell_size_meters=100):\n",
    "    \"\"\"\n",
    "    Map GPS coordinates to a grid cell index.\n",
    "\n",
    "    Parameters:\n",
    "    - lon, lat: GPS coordinates.\n",
    "    - min_lon, min_lat: Minimum longitude and latitude of the grid.\n",
    "    - num_rows, num_cols: Grid dimensions.\n",
    "    - cell_size_meters: Cell size.\n",
    "\n",
    "    Returns:\n",
    "    - int: Index of the cell.\n",
    "    \"\"\"\n",
    "    # Convert GPS coordinates to meters\n",
    "    R = 6371000\n",
    "    m_per_deg_lat = R * np.pi / 180\n",
    "    avg_lat = np.radians((min_lat + lat) / 2)\n",
    "    m_per_deg_lon = R * np.cos(avg_lat) * np.pi / 180\n",
    "\n",
    "    lat_m = (lat - min_lat) * m_per_deg_lat\n",
    "    lon_m = (lon - min_lon) * m_per_deg_lon\n",
    "\n",
    "    # Calculate row and column in the grid\n",
    "    row = int(lat_m / cell_size_meters)\n",
    "    col = int(lon_m / cell_size_meters)\n",
    "\n",
    "    # Ensure row and column are within the bounds of the grid\n",
    "    row = min(max(row, 0), num_rows - 1)\n",
    "    col = min(max(col, 0), num_cols - 1)\n",
    "\n",
    "    return row * num_cols + col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bd5aef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def slice_and_convert_polylines(df, start_index, end_index, gps_to_cell_index, min_lon, min_lat, max_lon, max_lat, num_rows, num_cols, directory_path):\n",
    "    \"\"\"\n",
    "    Slices a range of polylines from a dataframe, converts them into one-hot encoded vectors based on their geographical position, and saves the result to a JSON file.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the polyline data.\n",
    "    - start_index: The starting index of the slice.\n",
    "    - end_index: The ending index of the slice.\n",
    "    - gps_to_cell_index: A function that converts GPS coordinates to cell indices.\n",
    "    - min_lon: Minimum longitude of the area of interest.\n",
    "    - min_lat: Minimum latitude of the area of interest.\n",
    "    - max_lon: Maximum longitude of the area of interest.\n",
    "    - max_lat: Maximum latitude of the area of interest.\n",
    "    - num_rows: Number of rows in the grid representation.\n",
    "    - num_cols: Number of columns in the grid representation.\n",
    "    - directory_path: The directory path where the JSON file will be saved.\n",
    "    \n",
    "    The function will check if the directory exists and create it if necessary before saving the JSON file.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "    # Slicing polylines from the dataframe\n",
    "    sliced_polylines = df['POLYLINE'][start_index:end_index]\n",
    "    \n",
    "    # Convert polyline strings to one-hot encoded vectors\n",
    "    one_hot_series = polyline_to_onehot(sliced_polylines, gps_to_cell_index, min_lon, min_lat, max_lon, max_lat, num_rows, num_cols)\n",
    "    \n",
    "    json_str = one_hot_series.to_json()\n",
    "    \n",
    "    file_path = f'{directory_path}/one_hot_vectors_batch_{start_index}_{end_index}.json'  \n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(json_str)\n",
    "    \n",
    "    # Print saving information\n",
    "    print(f\"Saved batch from index {start_index} to {end_index} to file: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47ba015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def readCsvToDf(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df\n",
    "\n",
    "filtered_train_df = readCsvToDf('data/trainData/filtered_train_df.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ceabe250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch from index 0 to 100000 to file: data/trainData/oneHotVectorData/one_hot_vectors_batch_0_100000.json\n",
      "Saved batch from index 100000 to 200000 to file: data/trainData/oneHotVectorData/one_hot_vectors_batch_100000_200000.json\n",
      "Saved batch from index 200000 to 300000 to file: data/trainData/oneHotVectorData/one_hot_vectors_batch_200000_300000.json\n",
      "Saved batch from index 300000 to 400000 to file: data/trainData/oneHotVectorData/one_hot_vectors_batch_300000_400000.json\n",
      "Saved batch from index 400000 to 500000 to file: data/trainData/oneHotVectorData/one_hot_vectors_batch_400000_500000.json\n",
      "Saved batch from index 500000 to 600000 to file: data/trainData/oneHotVectorData/one_hot_vectors_batch_500000_600000.json\n",
      "Saved batch from index 600000 to 700000 to file: data/trainData/oneHotVectorData/one_hot_vectors_batch_600000_700000.json\n",
      "Saved batch from index 700000 to 800000 to file: data/trainData/oneHotVectorData/one_hot_vectors_batch_700000_800000.json\n",
      "Saved batch from index 800000 to 900000 to file: data/trainData/oneHotVectorData/one_hot_vectors_batch_800000_900000.json\n",
      "Saved batch from index 900000 to 1000000 to file: data/trainData/oneHotVectorData/one_hot_vectors_batch_900000_1000000.json\n",
      "Saved batch from index 1000000 to 1100000 to file: data/trainData/oneHotVectorData/one_hot_vectors_batch_1000000_1100000.json\n",
      "Saved batch from index 1100000 to 1200000 to file: data/trainData/oneHotVectorData/one_hot_vectors_batch_1100000_1200000.json\n",
      "Saved batch from index 1200000 to 1300000 to file: data/trainData/oneHotVectorData/one_hot_vectors_batch_1200000_1300000.json\n",
      "Saved batch from index 1300000 to 1400000 to file: data/trainData/oneHotVectorData/one_hot_vectors_batch_1300000_1400000.json\n",
      "Saved batch from index 1400000 to 1500000 to file: data/trainData/oneHotVectorData/one_hot_vectors_batch_1400000_1500000.json\n",
      "Saved batch from index 1500000 to 1600000 to file: data/trainData/oneHotVectorData/one_hot_vectors_batch_1500000_1600000.json\n",
      "Saved batch from index 1600000 to 1687544 to file: data/trainData/oneHotVectorData/one_hot_vectors_batch_1600000_1687544.json\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Convert Polylines to One-Hot Encoded Vectors\n",
    "min_lon = -8.710785 \n",
    "min_lat = 41.049459 \n",
    "max_lon = -8.510796 \n",
    "max_lat = 41.249448\n",
    "num_rows = 223\n",
    "num_cols = 168\n",
    "\n",
    "df_num_rows = len(filtered_train_df)\n",
    "batch_size = 100000\n",
    "for i in range(0, df_num_rows, batch_size):\n",
    "    start_index = i\n",
    "    end_index = min(i + batch_size, df_num_rows)\n",
    "    slice_and_convert_polylines(filtered_train_df, start_index, end_index, gps_to_cell_index=gps_to_cell_index, min_lon=min_lon, min_lat=min_lat, max_lon=max_lon, max_lat=max_lat, num_rows=num_rows, num_cols=num_cols, directory_path = 'data/trainData/oneHotVectorData')\n",
    "# slice_and_convert_polylines(filtered_train_df, start_index=0, end_index=len(filtered_train_df), gps_to_cell_index=gps_to_cell_index, min_lon=min_lon, min_lat=min_lat, max_lon=max_lon, max_lat=max_lat, num_rows=num_rows, num_cols=num_cols, directory_path = 'data/trainData')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eb225e",
   "metadata": {},
   "source": [
    "### same process of the test data\n",
    "Apply the same process to the test data: discard trajectories falling outside the boundary, and convert them into one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2e236e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 9)\n",
      "Dropping trajectory outside boundary for test data\n",
      "\n",
      "(316, 9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(test_df.shape)\n",
    "print(\"Dropping trajectory outside boundary for test data\", end=\"\\n\")\n",
    "filtered_test_df = drop_trajectory_outside_boundary(test_df, bounding_box)\n",
    "print()\n",
    "print(filtered_test_df.shape)\n",
    "\n",
    "filtered_test_df.to_csv('data/testData/filtered_test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2a933c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch from index 0 to 316 to file: data/testData/oneHotVectorData/one_hot_vectors_batch_0_316.json\n"
     ]
    }
   ],
   "source": [
    "slice_and_convert_polylines(filtered_test_df, start_index=0, end_index=len(filtered_test_df), gps_to_cell_index=gps_to_cell_index, min_lon=min_lon, min_lat=min_lat, max_lon=max_lon, max_lat=max_lat, num_rows=num_rows, num_cols=num_cols, directory_path = 'data/testData/oneHotVectorData')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67354917",
   "metadata": {},
   "source": [
    "### visualize the one-hot vectors\n",
    "Let's verify our approach by plotting the trajectory as one-hot vectors, with cells represented as white (for the cell with the corresponding cell_id) against a black background. From the sample plots, we can observe success: trajectories vary in length, with some extensive and others just a single dot. Overall, the plots appear reasonable and resemble a genuine trajectory pixel image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f11ba5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import random\n",
    "\n",
    "def plot_zoomed_cells_subplot(cell_ids, subplot_pos, num_rows=223, num_cols=168, padding=5, ax=None):\n",
    "    \"\"\"\n",
    "    Plots a zoomed-in image of a grid with specified IDs blacked out on a subplot.\n",
    "    \n",
    "    :param grid_ids: List of grid IDs to black out.\n",
    "    :param subplot_pos: Position of the subplot.\n",
    "    :param num_rows: Number of rows in the grid.\n",
    "    :param num_cols: Number of columns in the grid.\n",
    "    :param padding: Padding around the zoomed region for better visibility.\n",
    "    :param ax: Axes object to plot on.\n",
    "    \"\"\"\n",
    "    # Initialize the cell\n",
    "    cell = np.zeros((num_rows, num_cols))\n",
    "    \n",
    "    # Black out the specified cells\n",
    "    for cell_id in cell_ids:\n",
    "        row = cell_id // num_cols\n",
    "        col = cell_id % num_cols\n",
    "        cell[row, col] = 1\n",
    "\n",
    "    # Recalculate the positions to find the bounds\n",
    "    rows = [cell_id // num_cols for cell_id in cell_ids]\n",
    "    cols = [cell_id % num_cols for cell_id in cell_ids]\n",
    "\n",
    "    # Find the bounding box of the specified cells\n",
    "    min_row, max_row = min(rows), max(rows)\n",
    "    min_col, max_col = min(cols), max(cols)\n",
    "\n",
    "    # Add some padding to the bounding box for better visibility\n",
    "    min_row = max(0, min_row - padding)\n",
    "    max_row = min(num_rows - 1, max_row + padding)\n",
    "    min_col = max(0, min_col - padding)\n",
    "    max_col = min(num_cols - 1, max_col + padding)\n",
    "\n",
    "    # Zoom in on the specified region\n",
    "    zoomed_cell = cell[min_row:max_row+1, min_col:max_col+1]\n",
    "\n",
    "    # Plot on the specified subplot\n",
    "    ax.imshow(zoomed_grid, cmap='gray')\n",
    "    ax.axis('off')  # Hide the axes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "be0ba6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_zoomed_cells_subplot(cell_ids, subplot_pos, num_rows=223, num_cols=168, padding=5, ax=None):\n",
    "    \"\"\"\n",
    "    Plots a zoomed-in image of a grid with specified IDs in black (with alpha transparency) on a white background.\n",
    "    \n",
    "    :param cell_ids: List of cell IDs to highlight.\n",
    "    :param subplot_pos: Position of the subplot.\n",
    "    :param num_rows: Number of rows in the grid.\n",
    "    :param num_cols: Number of columns in the grid.\n",
    "    :param padding: Padding around the zoomed region for better visibility.\n",
    "    :param ax: Axes object to plot on.\n",
    "    \"\"\"\n",
    "    # Initialize the grid to white\n",
    "    grid = np.ones((num_rows, num_cols, 4))  # Use RGBA for the third dimension\n",
    "    \n",
    "    # Set specified cells to black with alpha=0.5\n",
    "    for cell_id in cell_ids:\n",
    "        row = cell_id // num_cols\n",
    "        col = cell_id % num_cols\n",
    "        grid[row, col] = [0, 0, 0, 0.5]  # RGBA for black with alpha 0.5\n",
    "\n",
    "    # Recalculate the positions to find the bounds\n",
    "    rows = [cell_id // num_cols for cell_id in cell_ids]\n",
    "    cols = [cell_id % num_cols for cell_id in cell_ids]\n",
    "\n",
    "    # Find the bounding box of the specified cells\n",
    "    min_row, max_row = min(rows), max(rows)\n",
    "    min_col, max_col = min(cols), max(cols)\n",
    "\n",
    "    # Add some padding to the bounding box for better visibility\n",
    "    min_row = max(0, min_row - padding)\n",
    "    max_row = min(num_rows - 1, max_row + padding)\n",
    "    min_col = max(0, min_col - padding)\n",
    "    max_col = min(num_cols - 1, max_col + padding)\n",
    "\n",
    "    # Zoom in on the specified region\n",
    "    zoomed_grid = grid[min_row:max_row+1, min_col:max_col+1]\n",
    "\n",
    "    # Plot on the specified subplot\n",
    "    if ax is not None:\n",
    "        ax.imshow(zoomed_grid)\n",
    "        ax.axis('off')  # Hide the axes\n",
    "    else:\n",
    "        plt.imshow(zoomed_grid)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "fb1cc8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "# Load data\n",
    "with open('data/testData/oneHotVectorData/one_hot_vectors_batch_0_316.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Create a figure for subplots\n",
    "fig, axs = plt.subplots(4, 3, figsize=(15, 10))\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "   \n",
    "    key = random.choice(list(data.keys()))\n",
    "    cell_ids = data[key]\n",
    "    plot_zoomed_cells_subplot(cell_ids, i, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "45744914",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/testData/oneHotVectorData/one_hot_df_batch_0_316.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k6/532x89vs5pb6w5twy9by4tl80000gn/T/ipykernel_43501/3014037038.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/testData/oneHotVectorData/one_hot_df_batch_0_316.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create a figure for subplots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/testData/oneHotVectorData/one_hot_df_batch_0_316.json'"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "with open('data/testData/oneHotVectorData/one_hot_df_batch_0_316.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Create a figure for subplots\n",
    "fig, axs = plt.subplots(4, 3, figsize=(15, 10))\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "   \n",
    "    key = random.choice(list(data.keys()))\n",
    "    cell_ids = data[key]\n",
    "    plot_zoomed_cells_subplot(cell_ids, i, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ac37e",
   "metadata": {
    "id": "795ac37e"
   },
   "source": [
    "## 5. Learning Trajectory Embedding Representation\n",
    "\n",
    "Given our objective of conducting a next POI prediction and similar trajectory search—two widely practiced tasks in data science—we will first delve into a fundamental problem --- learning embeddings representation for taxi trajectory data. Utilizing advanced deep learning techniques, we aim to learn a unified representation for taxi trajectory data that can support the two downstream tasks (next POI prediction and similarity search). The high-level idea is to use neural networks to learn low-dimensional embedding for taxi trajectory data such that two trajectories have similar spatial and temporal patterns if they are close in the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0645345b",
   "metadata": {
    "id": "0645345b"
   },
   "source": [
    "### 5.1 Importing relevant libraries\n",
    "We opt for the Pytorch libraries to learn and train a deep learning model that embeds taxi trajectory data into low-dimensional vector space. First, we need to import Pytorch libararies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1e7b49c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.2.1-cp39-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.17.1-cp39-cp39-macosx_10_13_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.2.1-cp39-cp39-macosx_10_13_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in /Users/pagewu/opt/anaconda3/lib/python3.9/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/pagewu/opt/anaconda3/lib/python3.9/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/pagewu/opt/anaconda3/lib/python3.9/site-packages (from torch) (1.9)\n",
      "Requirement already satisfied: networkx in /Users/pagewu/opt/anaconda3/lib/python3.9/site-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /Users/pagewu/opt/anaconda3/lib/python3.9/site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: fsspec in /Users/pagewu/opt/anaconda3/lib/python3.9/site-packages (from torch) (2021.8.1)\n",
      "Requirement already satisfied: numpy in /Users/pagewu/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (1.20.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/pagewu/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/pagewu/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/pagewu/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.2.1)\n",
      "Downloading torch-2.2.1-cp39-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.17.1-cp39-cp39-macosx_10_13_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.2.1-cp39-cp39-macosx_10_13_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-2.2.1 torchaudio-2.2.1 torchvision-0.17.1\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f260553",
   "metadata": {
    "id": "7f260553"
   },
   "source": [
    "### 5.2 Preparing and define the learning model\n",
    "Since spatial-temporal taxi trajectories are of time series data in nature, we choose Recurrent Neural Networks (RNN) as the model for embedding learning over taxi trajectories. Specifically, an RNN is a classical deep-learning model that can be trained to process a sequential data (or time-series data) input and produce a specific output sequence as prediction. It is typically done by transforming high-dimensional input time-series data into relatively low-dimensional hidden embebdding layers, and making predictions at the final output layer at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce1f538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9e2bb854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class TaxiTrajectoryGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, output_size):\n",
    "        super(TaxiTrajectoryGenerator, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = self.embedding(x)\n",
    "        x_packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out_packed, (h_n, c_n) = self.lstm(x_packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(out_packed, batch_first=True)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t_Zw7JHcULkJ",
   "metadata": {
    "id": "t_Zw7JHcULkJ"
   },
   "source": [
    "### 5.3 Preprocessing trajectory data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f721e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxiTrajectoryDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.input_sequences = [sequence[:-1] for sequence in sequences]\n",
    "        self.target_sequences = [sequence[1:] for sequence in sequences]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.input_sequences[idx]\n",
    "        target_seq = self.target_sequences[idx]\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "085e5074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import json\n",
    "\n",
    "def load_sequences_from_json(file_path, data_size):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file) \n",
    "    keys = list(data.keys())[:data_size]    \n",
    "    sequences = [data[str(key)] for key in keys]   \n",
    "    sequences = [[int(cell_id) for cell_id in sequence] for sequence in sequences]    \n",
    "    return sequences\n",
    "\n",
    "\n",
    "def prepare_dataset(directory_path, data_size, start=0, end=100000, step=100000, padding_value=0):\n",
    "    file_paths = [f\"{directory_path}/one_hot_vectors_batch_{i}_{i+step}.json\" for i in range(start, end, step)]\n",
    "    # file_paths.append(f\"{directory_path}/one_hot_vectors_batch_1600000_1687544.json\")  # Optionally add the last file if needed\n",
    "    \n",
    "    all_sequences = []\n",
    "    for file_path in file_paths:\n",
    "        sequences = load_sequences_from_json(file_path, data_size)\n",
    "        all_sequences.extend(sequences)\n",
    "    \n",
    "    tensor_sequences = [torch.tensor(sequence) for sequence in all_sequences]\n",
    "    padded_sequences = pad_sequence(tensor_sequences, batch_first=True, padding_value=padding_value)\n",
    "    \n",
    "    \n",
    "    return padded_sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6e32dc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'data/trainData/oneHotVectorData'\n",
    "dataset = prepare_dataset(directory_path, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a781ee6f",
   "metadata": {
    "id": "a781ee6f"
   },
   "source": [
    "### 5.5 Training the learning model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ae360d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def generate_kfold_indices(data, n_splits=3, shuffle=True, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate K-Fold indices for provided data.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The dataset to split. This function does not directly split the data but provides indices for splitting.\n",
    "    - n_splits: The number of folds. Default is 3.\n",
    "    - shuffle: Whether to shuffle the data before splitting into batches. Default is True.\n",
    "    - random_state: Seed used by the random number generator for reproducible output. Default is 42.\n",
    "\n",
    "    Returns:\n",
    "    - A list of tuples where each tuple contains the train and test indices for each fold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize KFold with the provided parameters\n",
    "    kf = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "    \n",
    "    # Generate and return the fold indices\n",
    "    folds = list(kf.split(data))\n",
    "    return folds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4a113778",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = generate_kfold_indices(padded_sequences, n_splits=3, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e134b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import os\n",
    "\n",
    "def cross_validate_model(dataset, folds, num_epochs=5, batch_size=8, lr=0.001, save_dir='trajectoryModels/testtestwaittodelete',padding_value = 0):\n",
    "    \"\"\"\n",
    "    Trains and validates a TaxiTrajectoryGenerator model using K-Fold cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: A list of sequences (data points) to be used for training and validation.\n",
    "    - folds: A list of tuples containing train and validation indices for each fold.\n",
    "    - num_epochs: Number of epochs to train for each fold. Default is 5.\n",
    "    - batch_size: Batch size for training and validation. Default is 8.\n",
    "    - lr: Learning rate for the optimizer. Default is 0.001.\n",
    "    - save_dir: Directory to save the trained models. Default is 'saved_models'.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple of lists containing training losses, validation losses, and accuracies for each fold.\n",
    "    \"\"\"\n",
    "    all_folds_train_loss = []\n",
    "    all_folds_val_loss = []\n",
    "    all_folds_accuracy = []\n",
    "\n",
    "    # Model specifications\n",
    "    vocab_size = 223 * 168\n",
    "    embedding_dim = 32\n",
    "    output_size = vocab_size\n",
    "    hidden_size = 32\n",
    "    num_layers = 2\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    for fold, (train_ids, test_ids) in enumerate(folds):\n",
    "        # Initialize fold-specific lists\n",
    "        fold_train_loss = []\n",
    "        fold_val_loss = []\n",
    "        fold_accuracy = []\n",
    "\n",
    "        # Split the sequences into training and validation sets\n",
    "        train_sequences = [dataset[i] for i in train_ids]\n",
    "        val_sequences = [dataset[i] for i in test_ids]\n",
    "\n",
    "        # Create the datasets and DataLoaders\n",
    "        train_dataset = TaxiTrajectoryDataset(train_sequences)\n",
    "        val_dataset = TaxiTrajectoryDataset(val_sequences)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Initialize the model, loss function, and optimizer\n",
    "        model = TaxiTrajectoryGenerator(vocab_size, embedding_dim, hidden_size, num_layers, output_size)\n",
    "        if torch.cuda.is_available():\n",
    "            model.cuda()\n",
    "#         loss_function = nn.CrossEntropyLoss()  \n",
    "        loss_function = nn.CrossEntropyLoss(ignore_index=padding_value)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Train the model on the current fold\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "\n",
    "            for sequences, targets in train_loader:\n",
    "                if torch.cuda.is_available():\n",
    "                    sequences, targets = sequences.cuda(), targets.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                lengths = [len(sequence) for sequence in sequences]  # Assuming your model takes sequence lengths\n",
    "                sequence_pred = model(sequences, lengths)  # Adjust according to your model's method signature\n",
    "                loss = loss_function(sequence_pred.transpose(1, 2), targets)  # Adjust if needed\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for sequences, targets in val_loader:\n",
    "                    if torch.cuda.is_available():\n",
    "                        sequences, targets = sequences.cuda(), targets.cuda()\n",
    "                    lengths = [len(sequence) for sequence in sequences]\n",
    "                    \n",
    "                    sequence_pred = model(sequences, lengths)\n",
    "                    loss = loss_function(sequence_pred.transpose(1, 2), targets)\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(sequence_pred, 2)\n",
    "                    # Create a mask for non-padding values\n",
    "                    non_padding_mask = targets != padding_value\n",
    "\n",
    "                    # Count correct predictions where the target is not a padding value\n",
    "                    correct_predictions += ((predicted == targets) & non_padding_mask).sum().item()\n",
    "\n",
    "                    # Update total predictions to exclude padding\n",
    "                    total_predictions += non_padding_mask.sum().item()\n",
    "#                     print('correct_predictions: ',correct_predictions,'; total_predictions: ',total_predictions)\n",
    "                   \n",
    "\n",
    "\n",
    "            accuracy = correct_predictions / total_predictions\n",
    "            \n",
    "            # Print fold, epoch, and validation loss\n",
    "            print(f'Fold {fold}, Epoch {epoch}, Train Loss: {train_loss / len(train_loader)}, Val Loss: {val_loss / len(val_loader)}, Accuracy: {accuracy}')\n",
    "    \n",
    "           \n",
    "            fold_train_loss.append(train_loss / len(train_loader))\n",
    "            fold_val_loss.append(val_loss / len(val_loader))\n",
    "            fold_accuracy.append(accuracy)\n",
    "\n",
    "        all_folds_train_loss.append(fold_train_loss)\n",
    "        all_folds_val_loss.append(fold_val_loss)\n",
    "        all_folds_accuracy.append(fold_accuracy)\n",
    "\n",
    "        # Save the model\n",
    "        model_save_path = os.path.join(save_dir, f'trajectory_model_fold_{fold}.pt')\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "    return all_folds_train_loss, all_folds_val_loss, all_folds_accuracy\n",
    "\n",
    "\n",
    "# def cross_validate_model(dataset, folds, num_epochs=5, batch_size=8, lr=0.001):\n",
    "#     \"\"\"\n",
    "#     Trains and validates a TaxiTrajectoryGenerator model using K-Fold cross-validation.\n",
    "\n",
    "#     Parameters:\n",
    "#     - dataset: A list of sequences (data points) to be used for training and validation.\n",
    "#     - folds: A list of tuples containing train and validation indices for each fold.\n",
    "#     - num_epochs: Number of epochs to train for each fold. Default is 5.\n",
    "#     - batch_size: Batch size for training and validation. Default is 8.\n",
    "#     - lr: Learning rate for the optimizer. Default is 0.001.\n",
    "\n",
    "#     Returns:\n",
    "#     - A tuple of lists containing training losses, validation losses, and accuracies for each fold.\n",
    "#     \"\"\"\n",
    "#     all_folds_train_loss = []\n",
    "#     all_folds_val_loss = []\n",
    "#     all_folds_accuracy = []\n",
    "\n",
    "#     # Model specifications\n",
    "#     vocab_size = 223 * 168\n",
    "#     embedding_dim = 128\n",
    "#     output_size = vocab_size\n",
    "#     hidden_size = 128\n",
    "#     num_layers = 2\n",
    "\n",
    "#     for fold, (train_ids, test_ids) in enumerate(folds):\n",
    "#         # Initialize fold-specific lists\n",
    "#         fold_train_loss = []\n",
    "#         fold_val_loss = []\n",
    "#         fold_accuracy = []\n",
    "\n",
    "#         # Split the sequences into training and validation sets\n",
    "#         train_sequences = [dataset[i] for i in train_ids]\n",
    "#         val_sequences = [dataset[i] for i in test_ids]\n",
    "\n",
    "#         # Create the datasets and DataLoaders\n",
    "#         train_dataset = TaxiTrajectoryDataset(train_sequences)\n",
    "#         val_dataset = TaxiTrajectoryDataset(val_sequences)\n",
    "#         train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#         val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#         # Initialize the model, loss function, and optimizer\n",
    "#         model = TaxiTrajectoryGenerator(vocab_size, embedding_dim, hidden_size, num_layers, output_size)\n",
    "#         loss_function = nn.CrossEntropyLoss()  \n",
    "#         optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#         # Train the model on the current fold\n",
    "#         for epoch in range(num_epochs):\n",
    "#             model.train()\n",
    "#             train_loss = 0\n",
    "#             correct_predictions = 0\n",
    "#             total_predictions = 0\n",
    "\n",
    "#             for sequences, targets in train_loader:\n",
    "#                 optimizer.zero_grad()\n",
    "#                 lengths = [len(sequence) for sequence in sequences]  # Assuming your model takes sequence lengths\n",
    "#                 sequence_pred = model(sequences, lengths)  # Adjust according to your model's method signature\n",
    "#                 loss = loss_function(sequence_pred.transpose(1, 2), targets)  # Adjust if needed\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 train_loss += loss.item()\n",
    "\n",
    "#             model.eval()\n",
    "#             val_loss = 0\n",
    "#             with torch.no_grad():\n",
    "#                 for sequences, targets in val_loader:\n",
    "#                     lengths = [len(sequence) for sequence in sequences]\n",
    "#                     sequence_pred = model(sequences, lengths)\n",
    "#                     loss = loss_function(sequence_pred.transpose(1, 2), targets)\n",
    "#                     val_loss += loss.item()\n",
    "#                     _, predicted = torch.max(sequence_pred, 2)\n",
    "#                     correct_predictions += (predicted == targets).sum().item()\n",
    "#                     total_predictions += targets.nelement()\n",
    "\n",
    "#             accuracy = correct_predictions / total_predictions\n",
    "            \n",
    "#             # Print fold, epoch, and validation loss\n",
    "#             print(f'Fold {fold}, Epoch {epoch}, Train Loss: {train_loss / len(train_loader)}, Val Loss: {val_loss / len(val_loader)}, Accuracy: {accuracy}')\n",
    "    \n",
    "#             fold_train_loss.append(train_loss / len(train_loader))\n",
    "#             fold_val_loss.append(val_loss / len(val_loader))\n",
    "#             fold_accuracy.append(accuracy)\n",
    "\n",
    "#         all_folds_train_loss.append(fold_train_loss)\n",
    "#         all_folds_val_loss.append(fold_val_loss)\n",
    "#         all_folds_accuracy.append(fold_accuracy)\n",
    "\n",
    "#     return all_folds_train_loss, all_folds_val_loss, all_folds_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2a01c67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k6/532x89vs5pb6w5twy9by4tl80000gn/T/ipykernel_43501/1362481826.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 0, Train Loss: 9.449348343743218, Val Loss: 7.610257625579834, Accuracy: 0.7269199346405228\n",
      "Fold 0, Epoch 1, Train Loss: 5.975944704479641, Val Loss: 4.49933934211731, Accuracy: 0.7285539215686274\n",
      "Fold 0, Epoch 2, Train Loss: 3.2474461131625705, Val Loss: 2.987789273262024, Accuracy: 0.7287581699346405\n",
      "Fold 0, Epoch 3, Train Loss: 2.41401199499766, Val Loss: 2.896636176109314, Accuracy: 0.7287581699346405\n",
      "Fold 0, Epoch 4, Train Loss: 2.1273718410068088, Val Loss: 2.902197074890137, Accuracy: 0.7287581699346405\n",
      "Fold 1, Epoch 0, Train Loss: 9.402845011817085, Val Loss: 7.338572025299072, Accuracy: 0.7699915824915825\n",
      "Fold 1, Epoch 1, Train Loss: 6.233848465813531, Val Loss: 4.010483169555664, Accuracy: 0.7727272727272727\n",
      "Fold 1, Epoch 2, Train Loss: 3.5093127621544733, Val Loss: 2.3735161781311036, Accuracy: 0.7729377104377104\n",
      "Fold 1, Epoch 3, Train Loss: 2.526863614718119, Val Loss: 2.2107461214065554, Accuracy: 0.7729377104377104\n",
      "Fold 1, Epoch 4, Train Loss: 2.401533391740587, Val Loss: 2.1943636775016784, Accuracy: 0.7729377104377104\n",
      "Fold 2, Epoch 0, Train Loss: 9.411964840359158, Val Loss: 7.57770004272461, Accuracy: 0.7666245791245792\n",
      "Fold 2, Epoch 1, Train Loss: 6.062133550643921, Val Loss: 4.210171413421631, Accuracy: 0.7687289562289562\n",
      "Fold 2, Epoch 2, Train Loss: 3.3411157661014133, Val Loss: 2.736609125137329, Accuracy: 0.7687289562289562\n",
      "Fold 2, Epoch 3, Train Loss: 2.5137728452682495, Val Loss: 2.6285420417785645, Accuracy: 0.7687289562289562\n",
      "Fold 2, Epoch 4, Train Loss: 2.353581574228075, Val Loss: 2.5887102603912355, Accuracy: 0.7687289562289562\n"
     ]
    }
   ],
   "source": [
    "all_train_loss, all_val_loss, all_accuracy = cross_validate_model(padded_sequences, folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e7580205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_validation_loss_and_accuracy(avg_train_loss, avg_val_loss, avg_accuracy):\n",
    "    \"\"\"\n",
    "    Plots the average training and validation loss, and the average accuracy over epochs.\n",
    "\n",
    "    Parameters:\n",
    "    - avg_train_loss: A list of average training losses over epochs.\n",
    "    - avg_val_loss: A list of average validation losses over epochs.\n",
    "    - avg_accuracy: A list of average accuracies over epochs.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plotting the average training and validation loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(avg_train_loss, label='Average Training Loss')\n",
    "    plt.plot(avg_val_loss, label='Average Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training & Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plotting the average accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(avg_accuracy, label='Average Accuracy', color='green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "38927cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training_validation_loss_and_accuracy(avg_train_loss, avg_val_loss, avg_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "726935da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k6/532x89vs5pb6w5twy9by4tl80000gn/T/ipykernel_43501/1362481826.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k6/532x89vs5pb6w5twy9by4tl80000gn/T/ipykernel_43501/1095158810.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_kfold_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mall_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_val_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/k6/532x89vs5pb6w5twy9by4tl80000gn/T/ipykernel_43501/4164892917.py\u001b[0m in \u001b[0;36mcross_validate_model\u001b[0;34m(dataset, folds, num_epochs, batch_size, lr, save_dir, padding_value)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Assuming your model takes sequence lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0msequence_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust according to your model's method signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3058\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3059\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "directory_path = 'data/trainData/oneHotVectorData'\n",
    "dataset = prepare_dataset(directory_path, 1000)\n",
    "folds = generate_kfold_indices(dataset, n_splits=3, shuffle=True, random_state=42)\n",
    "all_train_loss, all_val_loss, all_accuracy = cross_validate_model(dataset, folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f081b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_validation_loss_and_accuracy(avg_train_loss, avg_val_loss, avg_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "15a8b695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k6/532x89vs5pb6w5twy9by4tl80000gn/T/ipykernel_43501/1362481826.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 0, Train Loss: 10.527324464586046, Val Loss: 10.505381202697754, Accuracy: 0.0\n",
      "Fold 0, Epoch 1, Train Loss: 10.453518019782173, Val Loss: 10.466081809997558, Accuracy: 0.0\n",
      "Fold 0, Epoch 2, Train Loss: 10.302229669358995, Val Loss: 10.31632137298584, Accuracy: 0.0\n",
      "Fold 0, Epoch 3, Train Loss: 9.598952187432182, Val Loss: 9.799979782104492, Accuracy: 0.0007530120481927711\n",
      "Fold 0, Epoch 4, Train Loss: 8.466712209913466, Val Loss: 9.482545089721679, Accuracy: 0.0015060240963855422\n",
      "Fold 0, Epoch 5, Train Loss: 7.59634002049764, Val Loss: 9.533462142944336, Accuracy: 0.006024096385542169\n",
      "Fold 0, Epoch 6, Train Loss: 7.160115136040582, Val Loss: 9.694058418273926, Accuracy: 0.0007530120481927711\n",
      "Fold 0, Epoch 7, Train Loss: 6.973118570115831, Val Loss: 9.842063522338867, Accuracy: 0.006024096385542169\n",
      "Fold 0, Epoch 8, Train Loss: 6.9068815973069935, Val Loss: 9.962700462341308, Accuracy: 0.006024096385542169\n",
      "Fold 0, Epoch 9, Train Loss: 6.848419401380751, Val Loss: 10.068535232543946, Accuracy: 0.006024096385542169\n",
      "Fold 0, Epoch 10, Train Loss: 6.7793252733018665, Val Loss: 10.144239044189453, Accuracy: 0.006024096385542169\n",
      "Fold 0, Epoch 11, Train Loss: 6.722338411543104, Val Loss: 10.211751556396484, Accuracy: 0.006024096385542169\n",
      "Fold 0, Epoch 12, Train Loss: 6.751762602064344, Val Loss: 10.280879783630372, Accuracy: 0.006024096385542169\n",
      "Fold 0, Epoch 13, Train Loss: 6.710365401373969, Val Loss: 10.344826316833496, Accuracy: 0.006024096385542169\n",
      "Fold 0, Epoch 14, Train Loss: 6.709284994337294, Val Loss: 10.402259635925294, Accuracy: 0.006024096385542169\n",
      "Fold 0, Epoch 15, Train Loss: 6.628008789486355, Val Loss: 10.463388442993164, Accuracy: 0.006024096385542169\n",
      "Fold 0, Epoch 16, Train Loss: 6.679555310143365, Val Loss: 10.50587215423584, Accuracy: 0.004518072289156626\n",
      "Fold 0, Epoch 17, Train Loss: 6.647863652971056, Val Loss: 10.557324600219726, Accuracy: 0.004518072289156626\n",
      "Fold 0, Epoch 18, Train Loss: 6.6336653497484, Val Loss: 10.605828475952148, Accuracy: 0.004518072289156626\n",
      "Fold 0, Epoch 19, Train Loss: 6.673092736138238, Val Loss: 10.64279556274414, Accuracy: 0.006024096385542169\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k6/532x89vs5pb6w5twy9by4tl80000gn/T/ipykernel_43501/3175068073.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_kfold_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mall_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_val_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/k6/532x89vs5pb6w5twy9by4tl80000gn/T/ipykernel_43501/3787102848.py\u001b[0m in \u001b[0;36mcross_validate_model\u001b[0;34m(dataset, folds, num_epochs, batch_size, lr, save_dir, padding_value)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0msequence_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust according to your model's method signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "directory_path = 'data/trainData/oneHotVectorData'\n",
    "dataset = prepare_dataset(directory_path, 100)\n",
    "folds = generate_kfold_indices(dataset, n_splits=3, shuffle=True, random_state=42)\n",
    "all_train_loss, all_val_loss, all_accuracy = cross_validate_model(dataset, folds,num_epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b2463d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k6/532x89vs5pb6w5twy9by4tl80000gn/T/ipykernel_43501/1362481826.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 0, Train Loss: 10.534472783406576, Val Loss: 10.518127822875977, Accuracy: 0.0007530120481927711\n",
      "Fold 0, Epoch 1, Train Loss: 10.479749891493055, Val Loss: 10.49218807220459, Accuracy: 0.0007530120481927711\n",
      "Fold 0, Epoch 2, Train Loss: 10.408186700608995, Val Loss: 10.439935302734375, Accuracy: 0.0007530120481927711\n",
      "Fold 0, Epoch 3, Train Loss: 10.206404580010307, Val Loss: 10.292965316772461, Accuracy: 0.0007530120481927711\n",
      "Fold 0, Epoch 4, Train Loss: 9.742474661933052, Val Loss: 10.026091957092286, Accuracy: 0.0007530120481927711\n",
      "Fold 0, Epoch 5, Train Loss: 9.093077341715494, Val Loss: 9.754343605041504, Accuracy: 0.0007530120481927711\n",
      "Fold 0, Epoch 6, Train Loss: 8.4400634765625, Val Loss: 9.56874122619629, Accuracy: 0.0015060240963855422\n",
      "Fold 0, Epoch 7, Train Loss: 7.929052988688151, Val Loss: 9.492411422729493, Accuracy: 0.0030120481927710845\n",
      "Fold 0, Epoch 8, Train Loss: 7.6490086449517145, Val Loss: 9.48300838470459, Accuracy: 0.0030120481927710845\n",
      "Fold 0, Epoch 9, Train Loss: 7.449162006378174, Val Loss: 9.503693389892579, Accuracy: 0.0030120481927710845\n",
      "Fold 0, Epoch 10, Train Loss: 7.247814178466797, Val Loss: 9.540690803527832, Accuracy: 0.0030120481927710845\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k6/532x89vs5pb6w5twy9by4tl80000gn/T/ipykernel_43501/3175068073.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_kfold_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mall_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_val_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/k6/532x89vs5pb6w5twy9by4tl80000gn/T/ipykernel_43501/2338555901.py\u001b[0m in \u001b[0;36mcross_validate_model\u001b[0;34m(dataset, folds, num_epochs, batch_size, lr, save_dir, padding_value)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0msequence_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust according to your model's method signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "directory_path = 'data/trainData/oneHotVectorData'\n",
    "dataset = prepare_dataset(directory_path, 100)\n",
    "folds = generate_kfold_indices(dataset, n_splits=3, shuffle=True, random_state=42)\n",
    "all_train_loss, all_val_loss, all_accuracy = cross_validate_model(dataset, folds,num_epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "53f71cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k6/532x89vs5pb6w5twy9by4tl80000gn/T/ipykernel_43501/1362481826.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k6/532x89vs5pb6w5twy9by4tl80000gn/T/ipykernel_43501/3821806728.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_kfold_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mall_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_val_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/k6/532x89vs5pb6w5twy9by4tl80000gn/T/ipykernel_43501/2338555901.py\u001b[0m in \u001b[0;36mcross_validate_model\u001b[0;34m(dataset, folds, num_epochs, batch_size, lr, save_dir, padding_value)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0msequence_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust according to your model's method signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "directory_path = 'data/trainData/oneHotVectorData'\n",
    "dataset = prepare_dataset(directory_path, 1000)\n",
    "folds = generate_kfold_indices(dataset, n_splits=3, shuffle=True, random_state=42)\n",
    "all_train_loss, all_val_loss, all_accuracy = cross_validate_model(dataset, folds,num_epochs=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ec436",
   "metadata": {
    "id": "598ec436"
   },
   "source": [
    "### 5.6 Save the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa328bc7",
   "metadata": {
    "id": "fa328bc7"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_df_call_type_b = train_df.copy().dropna(subset=['ORIGIN_STAND'])\n",
    "selected_features_call_type_b = train_df_call_type_b[['ORIGIN_STAND', 'TAXI_ID', 'DAY_TYPE_REVISED_NUMERIC', 'TIMESTAMP']]\n",
    "plot_correlation_heatmap(selected_features_call_type_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb395d",
   "metadata": {
    "id": "96fb395d"
   },
   "source": [
    "The scatter plots show no clear patterns, indicating that how a taxi is called (from a center, stand, or street) and the time its trajectory is generated appear to be random. There's no apparent non-linear relationship, supporting the idea that these events are independent and occur without a predictable pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4985a93a",
   "metadata": {
    "id": "4985a93a"
   },
   "source": [
    "## 6. Next POI Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KjSoFTkd7UFQ",
   "metadata": {
    "id": "KjSoFTkd7UFQ"
   },
   "source": [
    "### 6.1 Preparing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67000e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "Tz149vDG7aoD",
   "metadata": {
    "id": "Tz149vDG7aoD"
   },
   "source": [
    "### 6.2 Predicting next POI using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd311863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict_next_cells(model, start_sequence, num_predictions):\n",
    "    \"\"\"\n",
    "    Predicts the next cell IDs in a trajectory given a starting sequence.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained TaxiTrajectoryGenerator model.\n",
    "    - start_sequence: The starting sequence of cell IDs as a tensor.\n",
    "    - num_predictions: The number of cell IDs to predict after the starting sequence.\n",
    "\n",
    "    Returns:\n",
    "    - A list containing the predicted sequence of cell IDs.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        current_sequence = start_sequence\n",
    "        predictions = []\n",
    "        for _ in range(num_predictions):\n",
    "            # Assuming your model expects lengths, you need to provide the current sequence length\n",
    "            lengths = [len(current_sequence)]\n",
    "            prediction = model(current_sequence.unsqueeze(0), lengths)  # Add batch dimension\n",
    "            # Assuming your output is logits and you want the most likely next cell ID\n",
    "            predicted_cell_id = prediction.argmax(dim=2)[:,-1]  # Get the last prediction\n",
    "            predictions.append(predicted_cell_id.item())  # Store the prediction\n",
    "            # Prepare the next input sequence; here, we simply append the prediction (you might need to adjust based on your input format)\n",
    "            current_sequence = torch.cat((current_sequence, predicted_cell_id), dim=0)\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8534fb56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaxiTrajectoryGenerator(\n",
       "  (embedding): Embedding(37464, 32)\n",
       "  (lstm): LSTM(32, 32, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=32, out_features=37464, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Embedding, LSTM, Linear\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Assuming the TaxiTrajectoryGenerator class definition is available in your environment\n",
    "\n",
    "# Model specifications - ensure these match those used during training\n",
    "vocab_size = 223 * 168  # Adjust if your vocab size is different\n",
    "embedding_dim = 32\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "output_size = vocab_size\n",
    "\n",
    "# Recreate the model instance\n",
    "model = TaxiTrajectoryGenerator(vocab_size, embedding_dim, hidden_size, num_layers, output_size)\n",
    "\n",
    "# Load the trained model weights\n",
    "model_path = 'trajectoryModels/model_fold_2.pt'\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))  # or 'cuda' if using GPU\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f88b25dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17546, 17714, 17713, 17712, 17710, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "111\n",
      "111\n"
     ]
    }
   ],
   "source": [
    "# Test trajectory\n",
    "test_trajectory_target = [17546,17714,17713,17712,17710,17708,17875,17875,18042,18042,18042,18042,18042,18042,18209,18209,18546,18715,18715,18715,18715,18883,18883,18883,18884,18884,18884,19052,19052,18884,19052,19052,19052,19052,19220,19220,19220,19220,19220,19220,19220,19220,19219,19219,19219,19218,19217,19217,19217,19217,19217,19216,19216,19214,19213,19213,19213,19380,19380,19717,20053,20222,20222,20390,20390,20390,20390,20390,20390,20558,20726,20893,21060,21060,21228,21228,21228,21228,21228,21228,21228,21228,21228,21228,21228,21228,21228,21229,21229,21229,21229,21229,21229,21229,21229,21397,21397,21229,21229,21229,21397,21398,21398,21398,21398,21398,21398,21565,21564,21564,21564]\n",
    "test_trajectory = test_trajectory_target[:5]\n",
    "for i in range(5, len(test_trajectory_target)):\n",
    "    test_trajectory.extend([0])\n",
    "print(test_trajectory)\n",
    "print(len(test_trajectory_target))\n",
    "print(len(test_trajectory))\n",
    "# Convert to tensor\n",
    "test_trajectory_tensor = torch.tensor(test_trajectory_target, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "98a5f5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17546, 17714, 17713, 17712, 17710, 17708, 17875, 17875, 18042, 18042,\n",
      "        18042, 18042, 18042, 18042, 18209, 18209, 18546, 18715, 18715, 18715,\n",
      "        18715, 18883, 18883, 18883, 18884, 18884, 18884, 19052, 19052, 18884,\n",
      "        19052, 19052, 19052, 19052, 19220, 19220, 19220, 19220, 19220, 19220,\n",
      "        19220, 19220, 19219, 19219, 19219, 19218, 19217, 19217, 19217, 19217,\n",
      "        19217, 19216, 19216, 19214, 19213, 19213, 19213, 19380, 19380, 19717,\n",
      "        20053, 20222, 20222, 20390, 20390, 20390, 20390, 20390, 20390, 20558,\n",
      "        20726, 20893, 21060, 21060, 21228, 21228, 21228, 21228, 21228, 21228,\n",
      "        21228, 21228, 21228, 21228, 21228, 21228, 21228, 21229, 21229, 21229,\n",
      "        21229, 21229, 21229, 21229, 21229, 21397, 21397, 21229, 21229, 21229,\n",
      "        21397, 21398, 21398, 21398, 21398, 21398, 21398, 21565, 21564, 21564,\n",
      "        21564])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Predicted next cell ID: 0\n"
     ]
    }
   ],
   "source": [
    "def predict_next_cell(model, sequence):\n",
    "    \"\"\"\n",
    "    Predict the next cell ID for a given sequence.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained TaxiTrajectoryGenerator model.\n",
    "    - sequence: The sequence of cell IDs as a tensor, with batch dimension.\n",
    "    \n",
    "    Returns:\n",
    "    - predicted_cell_id: The predicted next cell ID.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Assuming your model's forward method requires lengths\n",
    "        lengths = [len(sequence[0])]\n",
    "        print(sequence[0])\n",
    "        predictions = model(sequence, lengths)\n",
    "        print(predictions.argmax(dim=2))\n",
    "        # Assuming the last prediction is what we're interested in\n",
    "        predicted_cell_id = predictions.argmax(dim=2)[:,-3]  # Get the last prediction in the sequence\n",
    "        \n",
    "    return predicted_cell_id.item()\n",
    "\n",
    "# Use the function to predict the next cell ID\n",
    "next_cell_id = predict_next_cell(model, test_trajectory_tensor)\n",
    "print(f\"Predicted next cell ID: {next_cell_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fUe0hWJ7a2D",
   "metadata": {
    "id": "7fUe0hWJ7a2D"
   },
   "source": [
    "### 6.1 Caculating the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i0UxZBqV7bE-",
   "metadata": {
    "id": "i0UxZBqV7bE-"
   },
   "source": [
    "### 6.1 Visualzing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e619fed5",
   "metadata": {
    "id": "e619fed5"
   },
   "source": [
    "## 7. Efficient Similar Trajectory Search\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hu3PgrTN9sY2",
   "metadata": {
    "id": "Hu3PgrTN9sY2"
   },
   "source": [
    " ### 7.1 Load Langchian Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ovm7T5pM9siU",
   "metadata": {
    "id": "ovm7T5pM9siU"
   },
   "source": [
    " ### 7.2 Indexing learned trajectory embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6R6GrSRk9spd",
   "metadata": {
    "id": "6R6GrSRk9spd"
   },
   "source": [
    " ### 7.3 Search a trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FKI_LOaaAMn4",
   "metadata": {
    "id": "FKI_LOaaAMn4"
   },
   "source": [
    " ### 7.4 Measureing the efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7180f0",
   "metadata": {
    "id": "ef7180f0"
   },
   "source": [
    "## 8. Data Limitations\n",
    "\n",
    "**Data Completeness**:\n",
    "\n",
    "- The completeness of the GPS data stream varies for each trip. The \"MISSING_DATA\" field indicates whether there are missing GPS locations during the trip.\n",
    "- The reliability of the data may be affected if there are missing GPS coordinates.\n",
    "\n",
    "**Data Scope Limitation**:\n",
    "\n",
    "- The analysis is restricted to taxi trajectory data from a single city for a specific year due to computational constraints. However, the analytical procedures and techniques employed can be expanded to a broader context, encompassing multiple cities, spanning several years, and incorporating diverse transportation trajectories beyond taxis.\n",
    "- When applying the same methods to larger and more varied datasets, it is essential to consider the scalability and generalization of the analysis to different transportation contexts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6338225f",
   "metadata": {
    "id": "6338225f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b220284",
   "metadata": {
    "id": "3b220284"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e75f80",
   "metadata": {
    "id": "82e75f80"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
